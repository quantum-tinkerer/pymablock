\section{Perturbative block-diagonalization algorithm}

\subsection{Problem statement}

Pymablock finds a series of the unitary transformation $\mathcal{U}$ (we use calligraphic letters to denote series) that eliminates the off-diagonal components of the Hamiltonian
%
\begin{equation}
\label{eq:hamiltonian}
\mathcal{H} = H_0 + \mathcal{H}',
\end{equation}
%
with $\mathcal{H}' = \mathcal{H}'_{D} + \mathcal{H}'_{O}$ containing an arbitrary number and orders of perturbations with block-diagonal and block-offdiagonal components, respectively.
Here and later we use the subscipt $D$ to denote block-diagonal and $O$ to denote block-offdiagonal components of a series.
In different settings, diagonal and offdiagonal components may mean different things.
In the quasi-degenerate perturbation theory, the Hilbert space is subdivided into $A$ and $B$ subspaces, which makes $H_0$ a block-diagonal matrix
\begin{equation}
  H_0 = \begin{pmatrix}
    H_0^{AA} & 0 \\
    0 & H_0^{BB}
    \end{pmatrix},
\end{equation}
and the goal of the perturbation theory is to eliminate the offdiagonal $AB$ and $BA$ blocks of $\mathcal{H}$.
In the context of Rayleigh-Schr\"odinger perturbation theory, $H_0$ is a diagonal matrix, and the offdiagonal part of an operator are all its matrix elements that are not on the diagonal.

To consider the problem in the most general setting, we only require the diagonal and offdiagonal parts of an operator to satisfy the following constraints:
\begin{enumerate}
  \item The diagonal and offdiagonal parts of an operator add to identity: $\mathcal{A} = \mathcal{A}_{D} + \mathcal{A}_{O}$.
  \item Taking the diagonal or offdiagonal part of an operator is idempotent: $(\mathcal{A}_{D})_{D} = \mathcal{A}_{D}$.
  \item Taking the diagonal or offdiagonal part commutes with Hermitian conjugation: $(\mathcal{A}_{D})^\dagger = (\mathcal{A^\dagger})_{D}$.
  \item The offdiagonal part of any operator has no matrix elements within eigensubspaces of $H_0$.
\end{enumerate}
This definition is more more flexible than the intuitive concept of grouping an operator into blocks and separating those into diagonal and offdiagonal parts.
In particular, it allows to select any subset of the offdiagonal matrix elements as an offdiagonal part of an operator, as long as none of the matrix elements belong to an eigensubspace of $H_0$.

All the series we consider may be multivariate, and they represent sums of the form
%
\begin{equation}
\mathcal{A} = \sum_{n_1=0}^\infty \sum_{n_2=0}^\infty \cdots \sum_{n_k=0}^\infty \lambda_1^{n_1} \lambda_2^{n_2} \cdots \lambda_k^{n_k} A_{n_1, n_2, \ldots, n_k},
\end{equation}
%
where $\lambda_i$ are the perturbation parameters and $A_{n_1, n_2, \ldots, n_k}$ are linear operators.
%
The problem statement, therefore, is finding $\mathcal{U}$ and $\tilde{\mathcal{H}}$ such that
%
\begin{equation}
\label{eq:problem_definition}
\tilde{\mathcal{H}} = \mathcal{U}^\dagger \mathcal{H} \mathcal{U},\quad \tilde{\mathcal{H}}_{O} = 0,\quad \mathcal{U}^\dagger \mathcal{U} = 1,
\end{equation}
%
which is schematically shown in Fig.~\ref{fig:block_diagonalization}.
Series multiply according to the Cauchy product:
%
$$
\mathcal{C} = \mathcal{A}\mathcal{B} \Leftrightarrow C_\mathbf{n} = \sum_{\mathbf{m} + \mathbf{p} = \mathbf{n}} A_\mathbf{m} B_\mathbf{p}.
$$
%
The Cauchy product is the most expensive operation in perturbation theory, because it involves a large number of multiplications between potentially large matrices.
For example, evaluating $\mathbf{n}$-th order of $\mathcal{C}$ requires $\sim\prod_i n_i \equiv N$ multiplications of the series elements.\footnote{If both $\mathcal{A}$ and $\mathcal{B}$ are known in advance, fast Fourier transform-based algorithms can reduce this cost to $\sim N \log N$. In our problem, however, the series are constructed recursively and therefore this optimization is impossible.}
A direct computation of all the possible index combinations in a product between three series $\mathcal{A}\mathcal{B}\mathcal{C}$ would have a higher cost $\sim N^2$, however, if we use associativity of the product and compute this as $(\mathcal{A}\mathcal{B})\mathcal{C}$, then the scaling of the cost stays $\sim N$.

There are many ways to solve the problem~\eqref{eq:problem_definition} that give identical expressions for $\mathcal{U}$ and $\tilde{\mathcal{H}}$.
We are searching for a procedure that satisfies two additional constraints:
%
\begin{itemize}
    \item It has the same complexity scaling as a Cauchy product, and therefore
    $\sim N$ multiplications per additional order.
    \item It does not require multiplications by $H_0$.
    \item It requires only one Cauchy product by $\mathcal{H}_D$, the block-diagonal
    part of $\mathcal{H}$.
\end{itemize}
%
The first requirement is that the algorithm scaling is optimal: the desired expression at least contains a Cauchy product of $\mathcal{U}$ and $\mathcal{H}$.
Therefore the complexity scaling of the complete algorithm may not become lower than the complexity of a Cauchy product and we aim to reach this lower bound.
The second requirement is because in perturbation theory, $n$-th order corrections to $\tilde{\mathcal{H}}$ carry $n$ energy denominators $1/(E_i - E_j)$, where $E_i$ and $E_j$ are the eigenvalues of $H_0$ belonging to different subspaces.
Therefore, any additional multiplications by $H_0$ must cancel with additional energy denominators.
Multiplying by $H_0$ is therefore unnecessary work, and it gives longer intermediate expressions.
The third requirement we impose by considering a case in which $\mathcal{H}_{O}=0$, where $\mathcal{H}_D$ must at least enter $\tilde{\mathcal{H}}$ as an added term, without any products.
Moreover, because $\mathcal{U}$ depends on the entire Hamiltonian, there must be at least one Cauchy product by $\mathcal{H}'_D$.
The goal of our algorithm is thus to be efficient and to produce compact results that do not require further simplifications.

\subsection{Existing solutions}
\co{Pymablock's algorithm does not use the Schrieffer--Wolff transformation, because the former is inefficient.}
A common approach to constructing effective Hamiltonians in the $2\times 2$ block case is to use the Schrieffer--Wolff transformation~\cite{Schrieffer_1966}:
%
\begin{equation}
\tilde{\mathcal{H}} = e^\mathcal{S} \mathcal{H} e^{-\mathcal{S}}, \\
e^{\mathcal{S}} = 1 + \mathcal{S} + \frac{1}{2!} \mathcal{S} \mathcal{S}
+ \frac{1}{3!} \mathcal{S} \mathcal{S} \mathcal{S} + \cdots,
\end{equation}
%
where $\mathcal{S} = \sum_n S_n$ is an antihermitian polynomial series in the perturbative parameter, making $e^\mathcal{S}$ a unitary transformation.
Requiring that $\tilde{\mathcal{H}}^{AB} = 0$ gives a recursive equation for $S_n$, whose terms are nested commutators between the series of $\mathcal{S}$ and $\mathcal{H}$.
Similarly, the transformed Hamiltonian is given by a series of nested commutators
%
\begin{equation}
\label{eq:SW_H}
\tilde{\mathcal{H}} = \sum_{j=0}^\infty \frac{1}{j!} \Big [\mathcal{H}, \sum_{n=0}^{\infty} S_n \Big ]^{(j)},
\end{equation}
%
where the superscript $(j)$ denotes the $j$-th nested commutator $[A, B]^{(j)} = [[A, B]^{(j-1)}, B]$, with $[A, B]^{(0)} = A$ and $[A, B]^{(1)} = AB - BA$.
Regardless of the specific implementation, this expression does not meet either of our two requirements:
\begin{itemize}
  \item The direct computation of the series elements requires $\sim \exp N$ multiplications, and even an optimized one has a $\sim N^2$ scaling.
  \item Evaluating Eq.~\eqref{eq:SW_H} contains multiplications by $H_0$.
\end{itemize}

\co{There are algorithms that use different parametrizations for U a difference that is crucial for efficiency, even though the results are equivalent.}
Alternative parametrizations of the unitary transformation $\mathcal{U}$ require solving unitarity and block diagonalization conditions too, but give rise to a different recursive procedure for the series elements.
For example, using hyperbolic functions
%
\begin{gather}
\mathcal{U} = \cosh{\mathcal{G}} + \sinh{\mathcal{G}}, \quad
\mathcal{G} = \sum_{i=0}^{\infty} G_i,
\end{gather}
%
leads to different recursive expressions for $G_i$~\cite{Shavitt_1980}, but does not change the algorithm's complexity.
On the other hand, using a polynomial series directly
%
\begin{equation}
\mathcal{U} = \sum_{i=0}^{\infty} U_i,
\end{equation}
%
gives rise to another recursive equation for $U_i$~\cite{Van_Vleck_1929, Lowdin_1962, Klein_1974, Suzuki_1983}.
Still, this choice results in an expression for $\tilde{\mathcal{H}}$ whose terms include products by $H_0$, and therefore requires additional simplifications.

\co{The existing algorithms with linear scaling are not suitable for the construction of an effective Hamiltonian.}
The following three algorithms satisfy both of our requirements while solving a related problem.
First, density matrix perturbation theory~\cite{McWeeny_1962,McWeeny_1968,Truflandier_2020} constructs the density
matrix $\mathcal{\rho}$ of a perturbed system as a power series with respect to a perturbative parameter:
%
\begin{equation}
  \mathcal{\rho} = \sum_{i=0}^{\infty} \rho_i.
\end{equation}
%
The elements of the series are found by solving two recursive conditions, $\mathcal{\rho}^2 = \mathcal{\rho}$ and $[\mathcal{H}, \mathcal{\rho}]=0$, which avoid multiplications by $H_0$ and require a single Cauchy product each.
This approach, however, deals with the entire Hilbert space, rather than the low-energy subspace, and does not provide an effective Hamiltonian.
Second, the perturbative similarity transform by C.~Bloch~\cite{Bloch_1958,Bravyi_2011} constructs the effective Hamiltonian in a non-orthogonal basis, which preserves the Hamiltonian spectrum while breaking its hermiticity.
Finally, the recursive Schrieffer--Wolff algorithm~\cite{Li_2022} applies the Schrieffer--Wolff transformation to the output of lower-order iterations, and calculates the effective Hamiltonian at a fixed perturbation strength, rather
than as a series.
We thus identify the following open question: can we construct an effective Hamiltonian with a linear scaling algorithm that produces compact expressions?

\subsection{Pymablock's algorithm}
\label{sec:pymablock_algorithm}

\co{We use recursive expressions, for example, to apply the unitarity condition.}
The first idea that Pymablock exploits is the recursive evaluation of the operator series, which we illustrate by considering the unitarity condition.
Let us separate the transformation $\mathcal{U}$ into an identity and $\mathcal{U}' = \mathcal{W} + \mathcal{V}$:
%
\begin{equation}
\label{eq:U}
\mathcal{U} = 1 + \mathcal{U}' = 1 + \mathcal{W} + \mathcal{V},\quad \mathcal{W}^\dagger = \mathcal{W},\quad \mathcal{V}^\dagger = -\mathcal{V}.
\end{equation}
%
We use the unitarity condition $\mathcal{U}^\dagger \mathcal{U} = 1$ by substituting $\mathcal{U}'$ into it:
%
\begin{gather}
\label{eq:unitarity}
  1 = (1 + \mathcal{U}'^\dagger)(1+\mathcal{U}') = 1 + \mathcal{U}'^\dagger + \mathcal{U}' + \mathcal{U}'^\dagger \mathcal{U}'.
\end{gather}
%
This immediately yields \begin{gather}
\label{eq:W}
\mathcal{W} = \frac{1}{2}(\mathcal{U}'^\dagger + \mathcal{U}') = -\frac{1}{2} \mathcal{U}'^\dagger \mathcal{U}'.
\end{gather}
%
Because $\mathcal{U}'$ has no $0$-th order term, $(\mathcal{U}'^\dagger \mathcal{U}')_\mathbf{n}$ does not depend on the $\mathbf{n}$-th order of $\mathcal{U}'$ nor $\mathcal{W}$, and therefore Eq.~\eqref{eq:W} allows to compute $\mathcal{W}$ using the already available lower orders of $\mathcal{U}'$.
Alternatively, using Eq.~\eqref{eq:U} we could define $\mathcal{W}$ as a Taylor series in $\mathcal{V}$:
%
$$
\mathcal{W} = \sqrt{1 + \mathcal{V}^2} - 1 \equiv f(\mathcal{V}) \equiv \sum_n a_n \mathcal{V}^{2n}.
$$
%
A direct computation of all possible products of terms in this expression requires $\sim \exp N$ multiplications.
A more efficient approach for evaluating this expression introduces each term in the sum as a new series $\mathcal{A}^{n+1} = \mathcal{A}\mathcal{A}^{n}$ and reuses the previously computed results.
This optimization brings the exponential cost down to $\sim N^2$.
However, we see that the Taylor expansion approach is both more complicated and more computationally expensive than the recurrent definition in Eq.~\eqref{eq:W}.
Therefore, we use Eq.~\eqref{eq:W} to efficiently compute $\mathcal{W}$.
More generally, a Cauchy product $\mathcal{A}\mathcal{B}$ where $\mathcal{A}$ and $\mathcal{B}$ have no $0$-th order terms depends on $\mathcal{A}_1, \ldots, \mathcal{A}_{n-1}$ and $\mathcal{B}_1, \ldots, \mathcal{B}_{n-1}$.
This makes it possible to use $\mathcal{AB}$ in a recurrence relation, a property that we exploit throughout the algorithm.

\co{To fully define the unitary transformation, we make a choice for V.}
To compute $\mathcal{U}'$ we also need to find $\mathcal{V}$, which is defined by the requirement $\tilde{\mathcal{H}}^{AB} = 0$.
Additionally, we constrain $\mathcal{V}$ to be off-diagonal: $\mathcal{V}_{D} = 0$, a choice we make to minimize the norm of $\mathcal{U}'$~\cite{Cederbaum_1989}.
In the $2\times 2$ block case, this makes $\mathcal{W}$ block-diagonal and ensures that the resulting unitary transformation is equivalent to the Schrieffer--Wolff transformation (see section~\ref{seq:SW_equivalence}).

\co{We find V and the transformed Hamiltonian.}
The remaining condition for finding a recurrent relation for $\mathcal{U}'$ is that the transformed Hamiltonian
%
\begin{equation}
\label{eq:H_tilde_def}
\tilde{\mathcal{H}} = \mathcal{U}^\dagger \mathcal{H} \mathcal{U} = \mathcal{H}_{D} +
\mathcal{U}'^\dagger \mathcal{H}_{D} + \mathcal{H}_{D} \mathcal{U}' + \mathcal{U}'^\dagger \mathcal{H}_{D}
\mathcal{U}' + \mathcal{U}^\dagger\mathcal{H}'_{O}\mathcal{U},
\end{equation}
%
is block-diagonal, a condition that determines $\mathcal{V}$.
Here we used $\mathcal{U}=1+\mathcal{U}'$ and $\mathcal{H} = \mathcal{H}_{D} + \mathcal{H}'_{O}$, since $H_0$ is block-diagonal by definition.
Because we want to avoid products by $\mathcal{H}_{D}$, we need to get rid of the terms that contain it by replacing them with an alternative expression.
Our strategy is to define an auxiliary operator $\mathcal{X}$ that we can compute without ever multiplying by $\mathcal{H}_{D}$.
Like $\mathcal{U}'$, $\mathcal{X}$ needs to be defined via a recurrence relation, which we determine later.
Because Eq.~\eqref{eq:H_tilde_def} contains $\mathcal{H}_{D}$ multiplied by $\mathcal{U}'$ from the left and from the right, eliminating $\mathcal{H}_{D}$ requires moving it to the same side.
To achieve this, we choose $\mathcal{X}=\mathcal{Y}+\mathcal{Z}$ to be the commutator between $\mathcal{U}'$ and $\mathcal{H}_{D}$:
%
\begin{equation}
\label{eq:XYZ}
\mathcal{X} \equiv [\mathcal{U}', \mathcal{H}_{D}] = \mathcal{Y} + \mathcal{Z}, \quad
\mathcal{Y} \equiv [\mathcal{V}, \mathcal{H}_{D}] = \mathcal{Y}^\dagger,\quad
\mathcal{Z} \equiv [\mathcal{W}, \mathcal{H}_{D}] = -\mathcal{Z}^\dagger.
\end{equation}
%
In the common case, but not the most general case, when $A_D$ is a block-diagonal operator, $\mathcal{Y}$ is block off-diagonal.
Additionally, in the $2\times 2$ block case $\mathcal{Z}$ is block-diagonal.
We use $\mathcal{H}_{D} \mathcal{U}' = \mathcal{U}' \mathcal{H}_{D} -\mathcal{X}$ to move $\mathcal{H}_{D}$ through to the right and find
%
\begin{equation}
\label{eq:H_tilde}
\begin{aligned}
  \tilde{\mathcal{H}}
  &= \mathcal{H}_{D} + \mathcal{U}'^\dagger \mathcal{H}_{D} + (\mathcal{H}_{D} \mathcal{U}') + \mathcal{U}'^\dagger \mathcal{H}_{D}
  \mathcal{U}' + \mathcal{U}^\dagger(\mathcal{H}'_{O}\mathcal{U})
  \\
  &= \mathcal{H}_{D} + \mathcal{U}'^\dagger \mathcal{H}_{D} + \mathcal{U}'\mathcal{H}_{D} - \mathcal{X} + \mathcal{U}'^\dagger (\mathcal{U}' \mathcal{H}_{D} - \mathcal{X}) + \mathcal{U}^\dagger\mathcal{H}'_{O}\mathcal{U} \\
  &= \mathcal{H}_{D} + (\mathcal{U}'^\dagger + \mathcal{U}' + \mathcal{U}'^\dagger \mathcal{U}')\mathcal{H}_{D} - \mathcal{X} - \mathcal{U}'^\dagger \mathcal{X} + \mathcal{U}^\dagger\mathcal{H}'_{O}\mathcal{U} \\
  &= \mathcal{H}_{D} - \mathcal{X} - \mathcal{U}'^\dagger \mathcal{X} + \mathcal{U}^\dagger\mathcal{H}'_{O}\mathcal{U},
\end{aligned}
\end{equation}
%
where the terms multiplied by $\mathcal{H}_{D}$ cancel according to Eq.~\eqref{eq:unitarity}.
The transformed Hamiltonian does not contain multiplications by $\mathcal{H}_{D}$ anymore, but it does depend on $\mathcal{X}$, an auxiliary operator whose recurrent definition we do not know yet.
To find it, we first focus on its anti-Hermitian part, $\mathcal{Z}$.
Since recurrence relations are expressions whose right-hand side contains Cauchy products between series, we need to find a way to make a product appear.
We do so by using the unitarity condition $\mathcal{U}'^\dagger + \mathcal{U}' = -\mathcal{U}'^\dagger \mathcal{U}'$ to obtain the recursive definition of $\mathcal{Z}$:
%
\begin{equation}
\label{eq:Z}
\begin{aligned}
\mathcal{Z}
&= \frac{1}{2} (\mathcal{X} - \mathcal{X}^{\dagger}) \\
&= \frac{1}{2}\Big[ (\mathcal{U}' + \mathcal{U}'^{\dagger}) \mathcal{H}_{D} - \mathcal{H}_{D} (\mathcal{U}' + \mathcal{U}'^{\dagger}) \Big] \\
&= \frac{1}{2} \Big[ - \mathcal{U}'^{\dagger} (\mathcal{U}'\mathcal{H}_{D} - \mathcal{H}_{D} \mathcal{U}') + (\mathcal{U}'\mathcal{H}_{D} - \mathcal{H}_{D} \mathcal{U}')^{\dagger} \mathcal{U}' \Big] \\
&= \frac{1}{2} (-\mathcal{U}'^{\dagger} \mathcal{X} + \mathcal{X}^{\dagger} \mathcal{U}').
\end{aligned}
\end{equation}
%
Similar to computing $W_{\mathbf{n}}$, computing $Z_{\mathbf{n}}$ requires lower-orders of $\mathcal{X}$ and $\mathcal{U}'$.
Then, we compute the Hermitian part of $\mathcal{X}$ by requiring that $\tilde{\mathcal{H}}_{O} = 0$ in the Eq.~\eqref{eq:H_tilde} and find
%
\begin{equation}
\label{eq:Y_O}
\mathcal{Y}_O = (\mathcal{U}^\dagger \mathcal{H}'_{O} \mathcal{U} -
\mathcal{U}'^\dagger \mathcal{X} - \mathcal{Z})_{O}.
\end{equation}
%
Once again, despite $\mathcal{X}$ enters the right hand side, because all the terms lack \nth{0} order, this defines a recursive relation $\mathcal{Y}$.
To fix the diagonal part of $\mathcal{Y}$, we use its definition~\eqref{eq:XYZ}, which gives
\begin{equation}
  \label{eq:sylvester}
  [\mathcal{V}, H_0] = \mathcal{Y} - [\mathcal{V}, \mathcal{H}'_{D}],
\end{equation}
which is a continuous-time Lyapunov equation for $\mathcal{V}$.
In order for this equation to be satisfiable, the diagonal part of the right hand side must vanish, since the left hand side is purely offdiagonal.
Therefore we find the diagonal part of $\mathcal{Y}$:
%
\begin{equation}
\label{eq:Y_D}
\mathcal{Y}_D = [\mathcal{V}, \mathcal{H}'_{D}]_D,
\end{equation}
and it vanishes if the diagonal part corresponds to a block-diagonal matrix.

Having found $\mathcal{Y}$, determining $\mathcal{V}$ from Eq.~\eqref{eq:sylvester} is straightforward, and it needs to be solved only once for every new order.
In the eigenbasis of $H_0$, the solution of this equation is $V_{\mathbf{n}, ij} = (\mathcal{Y}_O - [\mathcal{V}, \mathcal{H}'_{D}]_O)_{\mathbf{n}, ij}/(E_i - E_j)$, where $E_i$ are the eigenvalues of $H_0$.

\co{The algorithm is complete.}
We now have the complete algorithm:
%
\begin{enumerate}
    \item Define series $\mathcal{U}'$ and $\mathcal{X}$ and make use of their block structure and Hermiticity.
    \item To define the diagonal blocks of $\mathcal{U}'$, use $\mathcal{W} = -\mathcal{U}'^\dagger\mathcal{U}'/2$.
    \item To find the off-diagonal blocks of $\mathcal{U}'$, solve Sylvester's equation \\ $[\mathcal{V}, H_0] = (\mathcal{Y} - [\mathcal{V}, \mathcal{H}'_{D}])_{O}$.
      This requires $\mathcal{X}$.
    \item To find the antihermitian part of $\mathcal{X}$, define $\mathcal{Z} = (-\mathcal{U}'^\dagger\mathcal{X} + \mathcal{X}^\dagger\mathcal{U}')/2$.
    \item For the Hermitian part of $\mathcal{X}$, use $\mathcal{Y} = (-\mathcal{U}'^\dagger\mathcal{X} + \mathcal{U}^\dagger\mathcal{H}'\mathcal{U})_{O} + [\mathcal{V}, \mathcal{H}'_D]_D$.
    \item  Compute the effective Hamiltonian as $\tilde{\mathcal{H}} = \mathcal{H}_{D} - \mathcal{X} - \mathcal{U}'^\dagger \mathcal{X} + \mathcal{U}^\dagger\mathcal{H}'_{O}\mathcal{U}$.
\end{enumerate}

\subsection{Equivalence to Schrieffer--Wolff transformation}
\label{seq:SW_equivalence}
\co{Our algorithm is equivalent to a Schrieffer--Wolff transformation}
Pymablock's algorithm and the Schrieffer--Wolff transformation both find a unitary transformation $\mathcal{U}$ such that $\tilde{\mathcal{H}}^{AB}=0$.
They are therefore equivalent up to a gauge choice in each subspace, $A$ and $B$.
We establish the equivalence between the two by demonstrating that this gauge choice is the same for both algorithms.
The Schrieffer--Wolff transformation uses $\mathcal{U} = \exp \mathcal{S}$, where $\mathcal{S} = -\mathcal{S}^\dagger$ and $\mathcal{S}_{AA} = \mathcal{S}_{BB} = 0$, this restriction makes the result unique~\cite{Bravyi_2011}.
On the other hand, our algorithm produces the unique block-diagonalizing transformation with a block structure $\mathcal{U}_{AA} = \mathcal{U}_{AA}^{\dag}$, $\mathcal{U}_{BB} = \mathcal{U}_{BB}^{\dag}$ and $\mathcal{U}_{AB} = -\mathcal{U}_{BA}^{\dag}$.
The uniqueness is a consequence of the construction of the algorithm, where calculating every order gives a unique solution satisfying these conditions.
To see that the two solutions are identical, we expand the Taylor series of $\exp \mathcal{S}$.
Every even order gives a Hermitian, block-diagonal matrix, while every odd order gives an anti-Hermitian block off-diagonal matrix, showing that $\exp \mathcal{S}$ has the same structure as $\mathcal{U}$ above.
The reverse statement about the structure of $\log \mathcal{U}$ can be seen similarly, using the Taylor series of the logarithm around $1$.
Using a series expansion is justified by the perturbative nature of the result, meaning that $\mathcal{S}$ is close to $0$ and $\mathcal{U}$ is close to $1$.
Because of the uniqueness of both results, we find that $\exp \mathcal{S}$ from conventional Schrieffer--Wolff transformation is identical to $\mathcal{U}$ found by our algorithm, which remains true if both power series are truncated at a finite order.

\subsection{Extra optimization: common subexpression elimination}
While the algorithm of Sec.~\ref{sec:pymablock_algorithm} satisfies our requirements, we improve it further by reusing products that are needed in several places, such that the total number of matrix multiplications is reduced.
Firstly, we rewrite the expressions for $\mathcal{Z}$ in Eq.~\eqref{eq:Z} and $\tilde{\mathcal{H}}$ in Eq.~\eqref{eq:H_tilde} by utilizing the Hermitian conjugate of $\mathcal{U}'^\dagger \mathcal{X}$ without recomputing it:
%
\begin{gather*}
\mathcal{Z} = \frac{1}{2}\left[(-\mathcal{U}'^\dagger \mathcal{X})- \textrm{h.c.}\right],\\
\tilde{\mathcal{H}} = \mathcal{H}_{D} + \mathcal{U}^\dagger \mathcal{H}'_{O} \mathcal{U} - (\mathcal{U}'^\dagger \mathcal{X} + \textrm{h.c.})/2 - \mathcal{Y}_D,
\end{gather*}
%
where $\textrm{h.c.}$ is the Hermitian conjugate, and $\mathcal{Z}$ drops out from $\tilde{\mathcal{H}}$ because it is antihermitian.
%
Additionally, we reuse the repeated $\mathcal{A} \equiv \mathcal{H}'_{O}\mathcal{U}'$ in
%
\begin{equation}
\label{eq:UHU}
\mathcal{U}^\dagger \mathcal{H}'_{O} \mathcal{U} = \mathcal{H}'_{O} + \mathcal{A} + \mathcal{A}^\dagger + \mathcal{U}'^\dagger \mathcal{A}.
\end{equation}
%
Next, we observe that some products from the $\mathcal{U}^{\dagger} \mathcal{H}_{O}\mathcal{U}$ term appear both in $\mathcal{X}$ in Eq.~\eqref{eq:Y_O} and in $\tilde{\mathcal{H}}$~\eqref{eq:H_tilde}.
%
To avoid recomputing these products, we introduce $\mathcal{B} = \mathcal{X} - \mathcal{H}'_{O} - \mathcal{A}$ and define the recursive algorithm using $\mathcal{B}$ instead of $\mathcal{X}$.
%
With this definition, we compute the off-diagonal blocks of $\mathcal{B}$ as:
%
\begin{equation}
\label{eq:B_offdiag}
\begin{aligned}
  \mathcal{B}_O &= \left[\mathcal{Y} + \mathcal{Z} - \mathcal{H}'_{O} - \mathcal{A} \right]_O\\
  &= \left[\mathcal{A}^\dagger + \mathcal{U}'^\dagger\mathcal{A} - \mathcal{U}'^\dagger \mathcal{X} \right]_O\\
  &= \left[\mathcal{U}'^\dagger\mathcal{H}'_{O} + \mathcal{U}'^\dagger\mathcal{A} - \mathcal{U}'^\dagger \mathcal{X} \right]_O\\
  &= -(\mathcal{U'}^\dagger \mathcal{B})_O,
\end{aligned}
\end{equation}
%
where we also used Eq.~\eqref{eq:Y_O} and the definition of $\mathcal{A}$.
The diagonal blocks of $\mathcal{B}$, on the other hand, are given by
%
\begin{equation}
\label{eq:B_diag}
\begin{aligned}
  \mathcal{B}_D &= \left[\mathcal{X} - \mathcal{H}'_{O} - \mathcal{A}\right]_D \\
  &= \left[\frac{1}{2}[(-\mathcal{U}'^\dagger \mathcal{X})- \textrm{h.c.}] + \mathcal{Y} - \mathcal{A}\right]_D \\
  &= \left[\frac{1}{2}[(-\mathcal{U}'^\dagger [\mathcal{X} - \mathcal{H}'_{O} - \mathcal{A}])- \textrm{h.c.}]+ \mathcal{Y} - \frac{1}{2}[\mathcal{A}^\dagger + \mathcal{A} ] + {\frac{1}{2}[( - \mathcal{U}'^\dagger\mathcal{A} ) - \textrm{h.c.}]}\right]_D, \\
  &= \left[\frac{1}{2}[(-\mathcal{U}'^\dagger \mathcal{B})- \textrm{h.c.}]+ \left[\mathcal{V}\mathcal{H}'_D + \textrm{h.c}\right] - \frac{1}{2}[\mathcal{A}^\dagger + \textrm{h.c.} ]\right]_D,
\end{aligned}
\end{equation}
%
where we used Eq.~\eqref{eq:Z} and that $\mathcal{U}'^\dagger \mathcal{A}$ is Hermitian.
%
Using $\mathcal{B}$ changes the relation for $\mathcal{V}^{AB}$ in Eq.~\eqref{eq:sylvester} to
\begin{equation}
\label{eq:sylvester_optimized}
[\mathcal{V},H_0] = \left(\mathcal{B} - \mathcal{H}' - \mathcal{A} - [\mathcal{V}, \mathcal{H}'_{D}]\right)_{O}.
\end{equation}
Finally, we combine Eq.~\eqref{eq:H_tilde}, Eq.~\eqref{eq:UHU}, Eq.~\eqref{eq:B_diag} and Eq.~\eqref{eq:B_offdiag} to obtain the final expression for the effective Hamiltonian:
%
\begin{equation}
\label{eq:H_tilde_optimized}
\tilde{\mathcal{H}}_D = \mathcal{H}_D + \frac{1}{2}\left[\mathcal{A} - \mathcal{U}'^\dagger \mathcal{B} + 2\mathcal{V}\mathcal{H}'_D + \textrm{h.c.}\right]_D.
\end{equation}
Together with the series $\mathcal{U}'$ in Eqs.~(\ref{eq:W},\ref{eq:sylvester_optimized}), $\mathcal{A} = \mathcal{H}'_{O}\mathcal{U}'$, and $\mathcal{B}$ in Eqs.~(\ref{eq:B_diag},\ref{eq:B_offdiag}), this equation defines the optimized algorithm.
