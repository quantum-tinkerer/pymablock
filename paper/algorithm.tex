\section{Perturbative block-diagonalization algorithm}
\label{sec:algorithm}

\subsection{Problem statement}

Pymablock finds a series of the unitary transformation $\mathcal{U}$ (we use calligraphic letters to denote series) that eliminates the off-diagonal components of the Hamiltonian
%
\begin{equation}
\label{eq:hamiltonian}
\mathcal{H} = H_0 + \mathcal{H}',
\end{equation}
%
with $\mathcal{H}' = \mathcal{H}'_{S} + \mathcal{H}'_{R}$ containing an arbitrary number and orders of perturbations with block-diagonal and block-offdiagonal components, respectively.
Here and later we use the subscript $S$ to denote the selected part and $R$ to denote remaining components of a series, with the goal of the perturbation theory to obtain a Hamiltonian with only the selected part.
In other words, we aim to find a unitary transformation $\mathcal{U}$ that cancels the remaining part of the Hamiltonian.
In different settings, selected and remaining parts may mean different things.
In quasi-degenerate perturbation theory, the Hilbert space is subdivided into $A$ and $B$ subspaces, which makes $H_0$ a block-diagonal matrix
\begin{equation}
  H_0 = \begin{pmatrix}
    {H_0}^{AA} & 0 \\
    0 & {H_0}^{BB}
    \end{pmatrix},
\end{equation}
and the goal of the perturbation theory is to eliminate the offdiagonal $AB$ and $BA$ blocks of $\mathcal{H}$.
In this case the selected part is the block-diagonal part, and the remaining part is the block-offdiagonal part.
Differently, in the context of Rayleigh-Schr\"odinger perturbation theory, $H_0$ is a diagonal matrix so that the selected part is the diagonal, and the remaining part of an operator are all its matrix elements that are not on the diagonal.

To consider the problem in the most general setting, we only require the selected and remaining parts of an operator to satisfy the following constraints:
\begin{enumerate}
  \item The selected and remaining parts of an operator add to identity: $\mathcal{A} = \mathcal{A}_{S} + \mathcal{A}_{R}$.
  \item Taking either part of an operator is idempotent: $(\mathcal{A}_{S})_{S} = \mathcal{A}_{S}$.
  \item Taking either part commutes with Hermitian conjugation: $(\mathcal{A}_{S})^\dagger = (\mathcal{A^\dagger})_{S}$.
  \item The remaining part of any operator has no matrix elements within eigensubspaces of $H_0$. This is required to ensure that the perturbation theory is well-defined.
\end{enumerate}
The separation of an operator into selected and remaining parts is a generalization of taking block-diagonal and block-offdiagonal parts.
In particular, the separation allows to choose any subset of the offdiagonal matrix elements as remaining, as long as none of the matrix elements belong to an eigensubspace of $H_0$.
That none of the matrix elements belong to a same eigensubspace of $H_0$ becomes evident in the textbook quasi-degenerate perturbation theory, where the corrections to energies and wavefunctions contain differences between energy of the states from different subspaces.
The main difference between our generalization and the standard separation into block-diagonal and block-offdiagonal is that the product of a selected part and remaining part of two operators may have a non-zero selected part: $(\mathcal{A}_{S}\mathcal{B}_{R})_{S} \neq 0$, while $(\mathcal{A}^{AA}\mathcal{B}^{AB})^{AA} = 0$.
The generality of the selected and remaining parts allows to consider all perturbation theory methods with the same algorithm, including multi-block diagonalization, selective diagonalization, and the Schrieffer--Wolff transformation.

All the series we consider may be multivariate, and they represent sums of the form
%
\begin{equation}
\mathcal{A} = \sum_{n_1=0}^\infty \sum_{n_2=0}^\infty \cdots \sum_{n_k=0}^\infty \lambda_1^{n_1} \lambda_2^{n_2} \cdots \lambda_k^{n_k} A_{n_1, n_2, \ldots, n_k},
\end{equation}
%
where $\lambda_i$ are the perturbation parameters and $A_{n_1, n_2, \ldots, n_k}$ are linear operators.
%
The problem statement, therefore, is finding $\mathcal{U}$ and $\tilde{\mathcal{H}}$ such that
%
\begin{equation}
\label{eq:problem_definition}
\tilde{\mathcal{H}} = \mathcal{U}^\dagger \mathcal{H} \mathcal{U},\quad \tilde{\mathcal{H}}_{R} = 0,\quad \mathcal{U}^\dagger \mathcal{U} = 1,
\end{equation}
%
which is schematically shown in Fig.~\ref{fig:block_diagonalization} for the case of two subspaces, where the selected parts are $AA$ and $BB$, and the remaining parts are $AB$ and $BA$.
Series multiply according to the Cauchy product:
%
$$
\mathcal{C} = \mathcal{A}\mathcal{B} \Leftrightarrow C_\mathbf{n} = \sum_{\mathbf{m} + \mathbf{p} = \mathbf{n}} A_\mathbf{m} B_\mathbf{p}.
$$
%
The Cauchy product is the most expensive operation in perturbation theory, because it involves a large number of multiplications between potentially large matrices.
For example, evaluating $\mathbf{n}$-th order of $\mathcal{C}$ requires $\sim\prod_i n_i \equiv N$ multiplications of the series elements.\footnote{If both $\mathcal{A}$ and $\mathcal{B}$ are known in advance, fast Fourier transform-based algorithms can reduce this cost to $\sim N \log N$. In our problem, however, the series are constructed recursively and therefore this optimization is impossible.}
A direct computation of all the possible index combinations in a product between three series $\mathcal{A}\mathcal{B}\mathcal{C}$ would have a higher cost $\sim N^2$, however, if we use associativity of the product and compute this as $(\mathcal{A}\mathcal{B})\mathcal{C}$, then the scaling of the cost stays $\sim N$.

There are many ways to solve the problem~\eqref{eq:problem_definition} that give identical expressions for $\mathcal{U}$ and $\tilde{\mathcal{H}}$.
We are searching for a procedure that satisfies two additional constraints:
%
\begin{itemize}
    \item It has the same complexity scaling as a Cauchy product, and therefore
    $\sim N$ multiplications per additional order.
    \item It does not require multiplications by $H_0$.
    \item It requires only one Cauchy product by $\mathcal{H}_{S}$, the selected
    part of $\mathcal{H}$.
\end{itemize}
%
The first requirement is that the algorithm scaling is optimal: the desired expression at least contains a Cauchy product of $\mathcal{U}$ and $\mathcal{H}$.
Therefore the complexity scaling of the complete algorithm may not become lower than the complexity of a Cauchy product and we aim to reach this lower bound.
The second requirement is because in perturbation theory, $n$-th order corrections to $\tilde{\mathcal{H}}$ carry $n$ energy denominators $1/(E_i - E_j)$, where $E_i$ and $E_j$ are the eigenvalues of $H_0$ belonging to different subspaces.
Therefore, any additional multiplications by $H_0$ must cancel with additional energy denominators.
Multiplying by $H_0$ is therefore unnecessary work, and it gives longer intermediate expressions.
The third requirement we impose by considering a case in which $\mathcal{H}_{R}=0$, where $\mathcal{H}_{S}$ must at least enter $\tilde{\mathcal{H}}$ as an added term, without any products.
Moreover, because $\mathcal{U}$ depends on the entire Hamiltonian, there must be at least one Cauchy product by $\mathcal{H}'_{S}$.
The goal of our algorithm is thus to be efficient and to produce compact results that do not require further simplifications.

\subsection{Existing solutions}
\co{Pymablock's algorithm does not use the Schrieffer--Wolff transformation, because the former is inefficient.}
A common approach to constructing effective Hamiltonians in the $2\times 2$ block case is to use the Schrieffer--Wolff transformation~\cite{Schrieffer_1966}:
%
\begin{equation}
\begin{aligned}
\tilde{\mathcal{H}} = e^\mathcal{S} \mathcal{H} e^{-\mathcal{S}}, \\
e^{\mathcal{S}} = 1 + \mathcal{S} + \frac{1}{2!} \mathcal{S} \mathcal{S}
+ \frac{1}{3!} \mathcal{S} \mathcal{S} \mathcal{S} + \cdots,
\end{aligned}
\end{equation}
%
where $\mathcal{S} = \sum_n S_n$ is an antihermitian polynomial series in the perturbative parameter, making $e^\mathcal{S}$ a unitary transformation.
Requiring that $\tilde{\mathcal{H}}^{AB} = 0$ gives a recursive equation for $S_n$, whose terms are nested commutators between the series of $\mathcal{S}$ and $\mathcal{H}$.
Similarly, the transformed Hamiltonian is given by a series of nested commutators
%
\begin{equation}
\label{eq:SW_H}
\tilde{\mathcal{H}} = \sum_{j=0}^\infty \frac{1}{j!} \Big [\mathcal{H}, \sum_{n=0}^{\infty} S_n \Big ]^{(j)},
\end{equation}
%
where the superscript $(j)$ denotes the $j$-th nested commutator $[A, B]^{(j)} = [[A, B]^{(j-1)}, B]$, with $[A, B]^{(0)} = A$ and $[A, B]^{(1)} = AB - BA$.
Regardless of the specific implementation, this expression does not meet either of our two requirements:
\begin{itemize}
  \item The direct computation of the series elements requires $\sim \exp N$ multiplications, and even an optimized one has a $\sim N^2$ scaling.
  \item Evaluating Eq.~\eqref{eq:SW_H} contains multiplications by $H_0$.
\end{itemize}
Additionally, while in the $2\times 2$ block case the Schrieffer--Wolff transformation produces a minimal unitary transformation, i.e. as close to identity as possible, this is not the case in the multi-block case~\cite{Mankodi_2024}.
The generalization of this approach to multiple subspaces is an open question~\cite{Mankodi_2024}.

\co{There are algorithms that use different parametrizations for U a difference that is crucial for efficiency, even though the results are equivalent.}
Alternative parametrizations of the unitary transformation $\mathcal{U}$ require solving unitarity and block diagonalization conditions too, but give rise to a different recursive procedure for the series elements.
For example, using hyperbolic functions
%
\begin{gather}
\mathcal{U} = \cosh{\mathcal{G}} + \sinh{\mathcal{G}}, \quad
\mathcal{G} = \sum_{i=0}^{\infty} G_i,
\end{gather}
%
leads to different recursive expressions for $G_i$~\cite{Shavitt_1980}, but does not change the algorithm's complexity.
On the other hand, using a polynomial series directly
%
\begin{equation}
\mathcal{U} = \sum_{i=0}^{\infty} U_i,
\end{equation}
%
gives rise to another recursive equation for $U_i$~\cite{Van_Vleck_1929, Lowdin_1962, Klein_1974, Suzuki_1983}.
Still, this choice results in an expression for $\tilde{\mathcal{H}}$ whose terms include products by $H_0$, and therefore requires additional simplifications.

\co{Continuous unitary transformations are another approach that relies on solving differential equations.}
Another approach uses Wegner's flow equation~\cite{Wegner_1994,Kehrein_2007} to construct a continuous unitary transformation (CUT) that depends smoothly on a fictitious parameter $l$, $\mathcal{U}(l)$.
The goal is to define a generator $\mathcal{\eta}(l)$ such that $\mathcal{H}(l) = \mathcal{U}^\dagger(l) \mathcal{H}(0) \mathcal{U}(l)$ flows towards the desired effective Hamiltonian:
%
\begin{equation}
\label{eq:hamiltonian_flow}
\frac{d\mathcal{H}(l)}{dl} = [\mathcal{\eta}(l), \mathcal{H}(l)],
\end{equation}
%
where $\mathcal{U}(l)$, $\mathcal{H}(l)$, and $\mathcal{\eta}(l)$ are once again series in the perturbative parameters.
At $l = \infty$, the transformed Hamiltonian does not contain the undesired terms, $\mathcal{H}(\infty) = \tilde{\mathcal{H}}$.
Finding the unitary amounts to solving a set of differential equations
%
\begin{equation}
\frac{d\mathcal{U}(l)}{dl} = \mathcal{\eta}(l)\mathcal{U}(l).
\end{equation}
%
Together with the Eq.~\eqref{eq:hamiltonian_flow} and an appropriate choice of $\mathcal{\eta}$, this gives a set of coupled differential equations, that become linear if solved order by order.
The convergence and stability of flow equations depends on the parameterization of the flow generator $\mathcal{\eta}$, and multiple strategies for this choice are known\todo{add citation}.
To apply the CUT method to second-quantized problems, one decomposes the perturbation into a set of quasiparticle creation and annihilation operators, $T_i$ and $T_i^\dagger$:
\begin{equation}
  \label{eq:eigenoperators}
  \mathcal{H} = H_0 + \sum_i \mathcal{D}_i (T_i + T_i^\dagger),\quad [H_0, T_i] = \omega_i T_i,
\end{equation}
with $\mathcal{D}_i$ a scalar series in the perturbative parameter, and $\omega_i$ the energies of the quasiparticles.
The CUT method is common in the study of many-body systems~\cite{Krull_2012}, and defining the generator benefits from decomposing the Hamiltonian into eigenoperators.

\co{The existing algorithms with linear scaling are not suitable for the construction of an effective Hamiltonian.}
The following three algorithms satisfy both of our requirements while solving a related problem.
First, density matrix perturbation theory~\cite{McWeeny_1962,McWeeny_1968,Truflandier_2020} constructs the density
matrix $\mathcal{\rho}$ of a perturbed system as a power series with respect to a perturbative parameter:
%
\begin{equation}
  \mathcal{\rho} = \sum_{i=0}^{\infty} \rho_i.
\end{equation}
%
The elements of the series are found by solving two recursive conditions, $\mathcal{\rho}^2 = \mathcal{\rho}$ and $[\mathcal{H}, \mathcal{\rho}]=0$, which avoid multiplications by $H_0$ and require a single Cauchy product each.
This approach, however, deals with the entire Hilbert space, rather than the low-energy subspace, and does not provide an effective Hamiltonian.
Second, the perturbative similarity transform by C.~Bloch~\cite{Bloch_1958,Bravyi_2011} constructs the effective Hamiltonian in a non-orthogonal basis, which preserves the Hamiltonian spectrum while breaking its hermiticity.
Third, the recursive Schrieffer--Wolff algorithm~\cite{Li_2022} applies the Schrieffer--Wolff transformation to the output of lower-order iterations, and calculates the effective Hamiltonian at a fixed perturbation strength, rather
than as a series.
Finally, none of the algorithms above handles more than two subspaces.
We thus identify the following open question: can we construct an effective Hamiltonian with a linear scaling algorithm that produces compact expressions?

\subsection{Pymablock's algorithm}
\label{sec:pymablock_algorithm}

\co{We use recursive expressions, for example, to apply the unitarity condition.}
The first idea that Pymablock exploits is the recursive evaluation of the operator series, which we illustrate by considering the unitarity condition.
Let us separate the transformation $\mathcal{U}$ into an identity and $\mathcal{U}' = \mathcal{W} + \mathcal{V}$:
%
\begin{equation}
\label{eq:U}
\mathcal{U} = 1 + \mathcal{U}' = 1 + \mathcal{W} + \mathcal{V},\quad \mathcal{W}^\dagger = \mathcal{W},\quad \mathcal{V}^\dagger = -\mathcal{V}.
\end{equation}
%
We use the unitarity condition $\mathcal{U}^\dagger \mathcal{U} = 1$ by substituting $\mathcal{U}'$ into it:
%
\begin{gather}
\label{eq:unitarity}
  1 = (1 + \mathcal{U}'^\dagger)(1+\mathcal{U}') = 1 + \mathcal{U}'^\dagger + \mathcal{U}' + \mathcal{U}'^\dagger \mathcal{U}'.
\end{gather}
%
This immediately yields \begin{gather}
\label{eq:W}
\mathcal{W} = \frac{1}{2}(\mathcal{U}'^\dagger + \mathcal{U}') = -\frac{1}{2} \mathcal{U}'^\dagger \mathcal{U}'.
\end{gather}
%
Because $\mathcal{U}'$ has no $0$-th order term, $(\mathcal{U}'^\dagger \mathcal{U}')_\mathbf{n}$ does not depend on the $\mathbf{n}$-th order of $\mathcal{U}'$ nor $\mathcal{W}$, and therefore Eq.~\eqref{eq:W} allows to compute $\mathcal{W}$ using the already available lower orders of $\mathcal{U}'$.
Alternatively, using Eq.~\eqref{eq:U} we could define $\mathcal{W}$ as a Taylor series in $\mathcal{V}$:
%
$$
\mathcal{W} = \sqrt{1 + \mathcal{V}^2} - 1 \equiv f(\mathcal{V}) \equiv \sum_n a_n \mathcal{V}^{2n}.
$$
%
A direct computation of all possible products of terms in this expression requires $\sim \exp N$ multiplications.
A more efficient approach for evaluating this expression introduces each term in the sum as a new series $\mathcal{A}^{n+1} = \mathcal{A}\mathcal{A}^{n}$ and reuses the previously computed results.
This optimization brings the exponential cost down to $\sim N^2$.
However, we see that the Taylor expansion approach is both more complicated and more computationally expensive than the recurrent definition in Eq.~\eqref{eq:W}.
Therefore, we use Eq.~\eqref{eq:W} to efficiently compute $\mathcal{W}$.
More generally, a Cauchy product $\mathcal{A}\mathcal{B}$ where $\mathcal{A}$ and $\mathcal{B}$ have no $0$-th order terms depends on $\mathcal{A}_1, \ldots, \mathcal{A}_{n-1}$ and $\mathcal{B}_1, \ldots, \mathcal{B}_{n-1}$.
This makes it possible to use $\mathcal{AB}$ in a recurrence relation, a property that we exploit throughout the algorithm.

\co{To fully define the unitary transformation, we make a choice for V.}
To compute $\mathcal{U}'$ we also need to find $\mathcal{V}$, which is defined by the requirement $\tilde{\mathcal{H}}_{R} = 0$.
Additionally, we constrain $\mathcal{V}$ to have no selected part: $\mathcal{V}_{S} = 0$, a choice we make to minimize the norm of $\mathcal{U}'$~\cite{Cederbaum_1989}.
That $\mathcal{V}_{S} = 0$ minimizes the norm of $\mathcal{U}'$ follows from the following statements:
\begin{enumerate}
  \item The norm of a series is minimal, when each of the subsequent terms is chosen to be minimal order by order.
  \item The Hermitian part of $\mathcal{U}'$, $W_\mathbf{n}$, is determined by the unitarity condition~\eqref{eq:W} at each order from lower orders of $\mathcal{U}'$.
  \item The norm of $W_\mathbf{n} + V_\mathbf{n}$ is minimal, when the norm of $V_\mathbf{n}$ is minimal because of Hermiticity properties of $\mathcal{W}$ and $\mathcal{V}$.
  \item Finally, because $\mathcal{V}_{R}$ is fixed by the requirement $\tilde{\mathcal{H}}_{R} = 0$, $\mathcal{V}_{S}=0$ provides the minimal norm of $\mathcal{U}'$.
\end{enumerate}
In the $2\times 2$ block case, this choice makes $\mathcal{W}$ block-diagonal and ensures that the resulting unitary transformation is equivalent to the Schrieffer--Wolff transformation (see section~\ref{seq:SW_equivalence}).
In general, however, $\mathcal{W}_{R} \neq 0$.

\co{We find V and the transformed Hamiltonian.}
The remaining condition for finding a recurrent relation for $\mathcal{U}'$ is that the transformed Hamiltonian
%
\begin{equation}
\label{eq:H_tilde_def}
\tilde{\mathcal{H}} = \mathcal{U}^\dagger \mathcal{H} \mathcal{U} = \mathcal{H}_{S} +
\mathcal{U}'^\dagger \mathcal{H}_{S} + \mathcal{H}_{S} \mathcal{U}' + \mathcal{U}'^\dagger \mathcal{H}_{S}
\mathcal{U}' + \mathcal{U}^\dagger\mathcal{H}'_{R}\mathcal{U},
\end{equation}
%
has only the selected part $\tilde{\mathcal{H}}_{R}=0$, a condition that determines $\mathcal{V}$.
Here we used $\mathcal{U}=1+\mathcal{U}'$ and $\mathcal{H} = \mathcal{H}_{S} + \mathcal{H}'_{R}$, since $H_0$ is has no remaining part by definition.
Because we want to avoid products by $\mathcal{H}_{S}$, we need to get rid of the terms that contain it by replacing them with an alternative expression.
Our strategy is to define an auxiliary operator $\mathcal{X}$ that we can compute without ever multiplying by $\mathcal{H}_{S}$.
Like $\mathcal{U}'$, $\mathcal{X}$ needs to be defined via a recurrence relation, which we determine later.
Because Eq.~\eqref{eq:H_tilde_def} contains $\mathcal{H}_{S}$ multiplied by $\mathcal{U}'$ from the left and from the right, eliminating $\mathcal{H}_{S}$ requires moving it to the same side.
To achieve this, we choose $\mathcal{X}=\mathcal{Y}+\mathcal{Z}$ to be the commutator between $\mathcal{U}'$ and $\mathcal{H}_{S}$:
%
\begin{equation}
\label{eq:XYZ}
\mathcal{X} \equiv [\mathcal{U}', \mathcal{H}_{S}] = \mathcal{Y} + \mathcal{Z}, \quad
\mathcal{Y} \equiv [\mathcal{V}, \mathcal{H}_{S}] = \mathcal{Y}^\dagger,\quad
\mathcal{Z} \equiv [\mathcal{W}, \mathcal{H}_{S}] = -\mathcal{Z}^\dagger.
\end{equation}
%
In the common case, but not the most general case, when the selected part is a block-diagonal operator, $\mathcal{Y}$ is block off-diagonal.
Additionally, in the $2\times 2$ block case $\mathcal{Z}$ is block-diagonal.
We use $\mathcal{H}_{S} \mathcal{U}' = \mathcal{U}' \mathcal{H}_{S} -\mathcal{X}$ to move $\mathcal{H}_{S}$ through to the right and find
%
\begin{equation}
\label{eq:H_tilde}
\begin{aligned}
  \tilde{\mathcal{H}}
  &= \mathcal{H}_{S} + \mathcal{U}'^\dagger \mathcal{H}_{S} + (\mathcal{H}_{S} \mathcal{U}') + \mathcal{U}'^\dagger \mathcal{H}_{S}
  \mathcal{U}' + \mathcal{U}^\dagger(\mathcal{H}'_{R}\mathcal{U})
  \\
  &= \mathcal{H}_{S} + \mathcal{U}'^\dagger \mathcal{H}_{S} + \mathcal{U}'\mathcal{H}_{S} - \mathcal{X} + \mathcal{U}'^\dagger (\mathcal{U}' \mathcal{H}_{S} - \mathcal{X}) + \mathcal{U}^\dagger\mathcal{H}'_{R}\mathcal{U} \\
  &= \mathcal{H}_{S} + (\mathcal{U}'^\dagger + \mathcal{U}' + \mathcal{U}'^\dagger \mathcal{U}')\mathcal{H}_{S} - \mathcal{X} - \mathcal{U}'^\dagger \mathcal{X} + \mathcal{U}^\dagger\mathcal{H}'_{R}\mathcal{U} \\
  &= \mathcal{H}_{S} - \mathcal{X} - \mathcal{U}'^\dagger \mathcal{X} + \mathcal{U}^\dagger\mathcal{H}'_{R}\mathcal{U},
\end{aligned}
\end{equation}
%
where the terms multiplied by $\mathcal{H}_{S}$ cancel according to Eq.~\eqref{eq:unitarity}.
The transformed Hamiltonian does not contain multiplications by $\mathcal{H}_{S}$ anymore, but it does depend on $\mathcal{X}$, an auxiliary operator whose recurrent definition we do not know yet.
To find it, we first focus on its anti-Hermitian part, $\mathcal{Z}$.
Since recurrence relations are expressions whose right-hand side contains Cauchy products between series, we need to find a way to make a product appear.
We do so by using the unitarity condition $\mathcal{U}'^\dagger + \mathcal{U}' = -\mathcal{U}'^\dagger \mathcal{U}'$ to obtain the recursive definition of $\mathcal{Z}$:
%
\begin{equation}
\label{eq:Z}
\begin{aligned}
\mathcal{Z}
&= \frac{1}{2} (\mathcal{X} - \mathcal{X}^{\dagger}) \\
&= \frac{1}{2}\Big[ (\mathcal{U}' + \mathcal{U}'^{\dagger}) \mathcal{H}_{S} - \mathcal{H}_{S} (\mathcal{U}' + \mathcal{U}'^{\dagger}) \Big] \\
&= \frac{1}{2} \Big[ - \mathcal{U}'^{\dagger} (\mathcal{U}'\mathcal{H}_{S} - \mathcal{H}_{S} \mathcal{U}') + (\mathcal{U}'\mathcal{H}_{S} - \mathcal{H}_{S} \mathcal{U}')^{\dagger} \mathcal{U}' \Big] \\
&= \frac{1}{2} (-\mathcal{U}'^{\dagger} \mathcal{X} + \mathcal{X}^{\dagger} \mathcal{U}').
\end{aligned}
\end{equation}
%
Similar to computing $W_{\mathbf{n}}$, computing $Z_{\mathbf{n}}$ requires lower-orders of $\mathcal{X}$ and $\mathcal{U}'$.
Then, we compute the Hermitian part of $\mathcal{X}$ by requiring that $\tilde{\mathcal{H}}_{R} = 0$ in the Eq.~\eqref{eq:H_tilde} and find
%
\begin{equation}
\label{eq:Y_R}
\mathcal{Y}_{R} = (\mathcal{U}^\dagger \mathcal{H}'_{R} \mathcal{U} -
\mathcal{U}'^\dagger \mathcal{X} - \mathcal{Z})_{R}.
\end{equation}
%
Once again, despite $\mathcal{X}$ enters the right hand side, because all the terms lack \nth{0} order, this defines a recursive relation $\mathcal{Y}$.
To fix $\mathcal{Y}_S$, we use its definition~\eqref{eq:XYZ}, which gives
\begin{equation}
  \label{eq:lyapunov}
  [\mathcal{V}, H_0] = \mathcal{Y} - [\mathcal{V}, \mathcal{H}'_{S}],
\end{equation}
which is a continuous-time Lyapunov equation for $\mathcal{V}$.
In order for this equation to be satisfiable, the selected part of the right hand side must vanish, since the left hand side has no selected part.
Therefore we find:
%
\begin{equation}
\label{eq:sylvester}
\mathcal{Y}_{S} = [\mathcal{V}, \mathcal{H}'_{S}]_{S},
\end{equation}
and it vanishes if the selected part corresponds to a block-diagonal matrix.

The final part is straightforward.
Finding $\mathcal{V}$ from $\mathcal{Y}$ amounts to solving a Sylvester's equation, Eq.~\ref{eq:sylvester}, which we only need to solve once for every new order.
This is the only step in the algorithm that requires a direct multiplication by $\mathcal{H}'_{S}$.
In the eigenbasis of $H_0$, the solution of Sylvester's equation is $V_{\mathbf{n}, ij} = (\mathcal{Y}_{R} - [\mathcal{V},
\mathcal{H}'_{S}]_{R})_{\mathbf{n}, ij}/(E_i - E_j)$, where $E_i$ are the eigenvalues of $H_0$.
However, even if the eigenbasis of $H_0$ is not available, there are efficient numerical algorithms to solve Sylvester's equation (see Sec.~\ref{sec:implicit}).
An alternative is to use the eigenoperator decomposition, an approach that does not require computing the eigenbasis of $H_0$, and therefore it is better suited for second quantized Hamiltonians~\cite{Landi_2024,Reascos_2024}.

\co{The algorithm is complete.}
We now have the complete algorithm:
%
\begin{enumerate}
    \item Define series $\mathcal{U}'$ and $\mathcal{X}$ and make use of their block structure and Hermiticity.
    \item To define the hermitian part of $\mathcal{U}'$, use $\mathcal{W} = -\mathcal{U}'^\dagger\mathcal{U}'/2$.
    \item To find the antihermitian part of $\mathcal{U}'$, solve Sylvester's equation \\ $[\mathcal{V}, H_0] = (\mathcal{Y} - [\mathcal{V}, \mathcal{H}'_{S}])_{R}$.
      This requires $\mathcal{X}$.
    \item To find the antihermitian part of $\mathcal{X}$, define $\mathcal{Z} = (-\mathcal{U}'^\dagger\mathcal{X} + \mathcal{X}^\dagger\mathcal{U}')/2$.
    \item For the Hermitian part of $\mathcal{X}$, use $\mathcal{Y} = (-\mathcal{U}'^\dagger\mathcal{X} + \mathcal{U}^\dagger\mathcal{H}'\mathcal{U})_{R} + [\mathcal{V}, \mathcal{H}'_{S}]_{S}$.
    \item  Compute the effective Hamiltonian as $\tilde{\mathcal{H}} \equiv \tilde{\mathcal{H}}_{S} = \mathcal{H}_{S} - \mathcal{X} - \mathcal{U}'^\dagger \mathcal{X} + \mathcal{U}^\dagger\mathcal{H}'_{R}\mathcal{U}$.
\end{enumerate}

\subsection{Equivalence to Schrieffer--Wolff transformation}
\label{seq:SW_equivalence}
\co{Our algorithm is equivalent to a Schrieffer--Wolff transformation}
Pymablock's algorithm applied to $2\times 2$ block-diagonalization and the Schrieffer--Wolff transformation both find a unitary transformation $\mathcal{U}$ such that $\tilde{\mathcal{H}}_{R} = \tilde{\mathcal{H}}^{AB}=0$.
They are therefore equivalent up to a gauge choice in each subspace, $A$ and $B$.
We establish the equivalence between the two by demonstrating that this gauge choice is the same for both algorithms.
The Schrieffer--Wolff transformation uses $\mathcal{U} = \exp \mathcal{S}$, where $\mathcal{S} = -\mathcal{S}^\dagger$ and $\mathcal{S}^{AA} = \mathcal{S}^{BB} = 0$, this restriction makes the result unique~\cite{Bravyi_2011}.
On the other hand, our algorithm produces the unique block-diagonalizing transformation with a block structure $\mathcal{U}^{AA} = {\mathcal{U}^{AA}}^{\dag}$, $\mathcal{U}^{BB} = {\mathcal{U}^{BB}}^{\dag}$ and $\mathcal{U}^{AB} = -\mathcal{U}_{BA}^{\dag}$.
The uniqueness is a consequence of the construction of the algorithm, where calculating every order gives a unique solution satisfying these conditions.
To see that the two solutions are identical, we expand $\exp \mathcal{S}$ into Taylor series.
In the resulting series every term containing a product of an even number of terms of $\mathcal{S}$ is a Hermitian, block-diagonal matrix, while every term containing a product of an odd number of terms of $\mathcal{S}$ is an anti-Hermitian block off-diagonal matrix.
Therefore $\exp \mathcal{S}$ has the same structure as $\mathcal{U}$ above.
Because both series are fixed by the hermiticity constraints on their block structure, we conclude that $\exp \mathcal{S}$ from conventional Schrieffer--Wolff transformation is identical to $\mathcal{U}$ found by our algorithm.

\subsection{Extra optimization: common subexpression elimination}
While the algorithm of Sec.~\ref{sec:pymablock_algorithm} satisfies our requirements, we improve it further by reusing products that are needed in several places, such that the total number of matrix multiplications is reduced.
Firstly, we rewrite the expressions for $\mathcal{Z}$ in Eq.~\eqref{eq:Z} and $\tilde{\mathcal{H}}$ in Eq.~\eqref{eq:H_tilde} by utilizing the Hermitian conjugate of $\mathcal{U}'^\dagger \mathcal{X}$ without recomputing it:
%
\begin{gather*}
\mathcal{Z} = \frac{1}{2}\left[(-\mathcal{U}'^\dagger \mathcal{X})- \textrm{h.c.}\right],\\
\tilde{\mathcal{H}} = \mathcal{H}_{S} + \mathcal{U}^\dagger \mathcal{H}'_{R} \mathcal{U} - (\mathcal{U}'^\dagger \mathcal{X} + \textrm{h.c.})/2 - \mathcal{Y}_{S},
\end{gather*}
%
where $\textrm{h.c.}$ is the Hermitian conjugate, and $\mathcal{Z}$ drops out from $\tilde{\mathcal{H}}$ because it is antihermitian.
%
Additionally, we reuse the repeated $\mathcal{A} \equiv \mathcal{H}'_{R}\mathcal{U}'$ in
%
\begin{equation}
\label{eq:UHU}
\mathcal{U}^\dagger \mathcal{H}'_{R} \mathcal{U} = \mathcal{H}'_{R} + \mathcal{A} + \mathcal{A}^\dagger + \mathcal{U}'^\dagger \mathcal{A}.
\end{equation}
%
Next, we observe that some products from the $\mathcal{U}^{\dagger} \mathcal{H}_{R}\mathcal{U}$ term appear both in $\mathcal{X}$ in Eq.~\eqref{eq:Y_R} and in $\tilde{\mathcal{H}}$~\eqref{eq:H_tilde}.
%
To avoid recomputing these products, we introduce $\mathcal{B} = \mathcal{X} - \mathcal{H}'_{R} - \mathcal{A}$ and define the recursive algorithm using $\mathcal{B}$ instead of $\mathcal{X}$.
%
With this definition, we compute the remaining part of $\mathcal{B}$ as:
%
\begin{equation}
\label{eq:B_offdiag}
\begin{aligned}
  \mathcal{B}_{R} &= \left[\mathcal{Y} + \mathcal{Z} - \mathcal{H}'_{R} - \mathcal{A} \right]_{R}\\
  &= \left[\mathcal{A}^\dagger + \mathcal{U}'^\dagger\mathcal{A} - \mathcal{U}'^\dagger \mathcal{X} \right]_{R}\\
  &= \left[\mathcal{U}'^\dagger\mathcal{H}'_{R} + \mathcal{U}'^\dagger\mathcal{A} - \mathcal{U}'^\dagger \mathcal{X} \right]_{R}\\
  &= -(\mathcal{U'}^\dagger \mathcal{B})_{R},
\end{aligned}
\end{equation}
%
where we also used Eq.~\eqref{eq:Y_R} and the definition of $\mathcal{A}$.
The selected part of $\mathcal{B}$, on the other hand, is given by
%
\begin{equation}
\label{eq:B_diag}
\begin{aligned}
  \mathcal{B}_{S} &= \left[\mathcal{X} - \mathcal{H}'_{R} - \mathcal{A}\right]_{S} \\
  &= \left[\frac{1}{2}[(-\mathcal{U}'^\dagger \mathcal{X})- \textrm{h.c.}] + \mathcal{Y} - \mathcal{A}\right]_{S} \\
  &= \left[\frac{1}{2}[(-\mathcal{U}'^\dagger [\mathcal{X} - \mathcal{H}'_{R} - \mathcal{A}])- \textrm{h.c.}]+ \mathcal{Y} - \frac{1}{2}[\mathcal{A}^\dagger + \mathcal{A} ] + {\frac{1}{2}[( - \mathcal{U}'^\dagger\mathcal{A} ) - \textrm{h.c.}]}\right]_{S}, \\
  &= \left[\frac{1}{2}[(-\mathcal{U}'^\dagger \mathcal{B})- \textrm{h.c.}]+ \left[\mathcal{V}\mathcal{H}'_{S} + \textrm{h.c}\right] - \frac{1}{2}[\mathcal{A}^\dagger + \textrm{h.c.} ]\right]_{S},
\end{aligned}
\end{equation}
%
where we used Eq.~\eqref{eq:Z} and that $\mathcal{U}'^\dagger \mathcal{A}$ is Hermitian.
%
Using $\mathcal{B}$ changes the relation for $\mathcal{V}$ in Eq.~\eqref{eq:sylvester} to
\begin{equation}
\label{eq:sylvester_optimized}
[\mathcal{V},H_0] = \left(\mathcal{B} - \mathcal{H}' - \mathcal{A} - [\mathcal{V}, \mathcal{H}'_{S}]\right)_{R}.
\end{equation}
Finally, we combine Eq.~\eqref{eq:H_tilde}, Eq.~\eqref{eq:UHU}, Eq.~\eqref{eq:B_diag} and Eq.~\eqref{eq:B_offdiag} to obtain the final expression for the effective Hamiltonian:
%
\begin{equation}
\label{eq:H_tilde_optimized}
\tilde{\mathcal{H}}_{S} = \mathcal{H}_{S} + \frac{1}{2}\left[\mathcal{A} - \mathcal{U}'^\dagger \mathcal{B} + 2\mathcal{V}\mathcal{H}'_{S} + \textrm{h.c.}\right]_{S}.
\end{equation}
Together with the series $\mathcal{U}'$ in Eqs.~(\ref{eq:W},\ref{eq:sylvester_optimized}), $\mathcal{A} = \mathcal{H}'_{R}\mathcal{U}'$, and $\mathcal{B}$ in Eqs.~(\ref{eq:B_diag},\ref{eq:B_offdiag}), this equation defines the optimized algorithm.
