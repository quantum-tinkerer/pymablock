\section{Implementation}

\co{To implement the algorithms, we need a data structure that represents a
multidimensional series of block matrices.}
To implement the algorithm, we need a data structure that represents a
multidimensional series of operators, where dimensions label independent
perturbations.
Additionally, the data structure needs to label blocks, so that the algorithm
supports several forms of input, e.g. dense arrays, sparse matrices, symbolic
expressions, an implicit subspace, or a custom Python object that supports
products and sums.
Manipulating blocks also allows to compute the effective Hamiltonian without
explicitly constructing the full Hamiltonian, which is useful for Hamiltonians
with a large $BB$ subspace that is costly to store and compute.
To run the recursion, the series needs to be queryable by order and block.
This is also useful in cases where the user may want terms that combine
different perturbations, or when the user wants to compute more terms than
originally requested.
Lastly, the data structure needs to support a block-wise multivariate Cauchy
product, which is the main operation in the recursion and is used to compute
the transformed Hamiltonian.

\co{We address this by defining a BlockSeries class.}
To address these requirements, we define a \mintinline{python}|BlockSeries|
Python class and use it to represent the series of $\mathcal{U}$,
$\mathcal{H}$, and $\tilde{\mathcal{H}}$.
A \mintinline{python}|BlockSeries| is a Python object equipped with a function
to compute its elements and a dictionary to cache the results.
For example, the \mintinline{python}|BlockSeries| for $\tilde{\mathcal{H}}$ has
a function that computes the block-wise multivariate Cauchy product in
Eq.~\eqref{eq:H_tilde_optimized}, \mintinline{python}|compute_H_tilde|.
%
\begin{minted}{python}
    H_tilde = BlockSeries(
        shape=(2, 2), # 2 blocks
        n_infinite=1, # number of perturbative parameters
        eval=compute_H_tilde,
    )
\end{minted}
%
To get the elements of the series, we implement Numpy array indexing,
which allows us to request several elements at once by using tuples and slices.
\mintinline{python}|H_tilde[0, 0, 2]| returns the $AA$ block of
$\tilde{\mathcal{H}}$ at order 2 in the perturbation, and
\mintinline{python}|H_tilde[0, 0, :3]| returns the $AA$ block of all orders up
to 2 in a Numpy masked array.
In the latter, the masked array only contains non-zero elements, a feature that
we use each time we compute the Cauchy products between series, avoiding
unnecessary operations.

\co{Using the BlockSeries interface allows us to implement a range of
optimizations that go beyond directly implementing the polynomial
parametrization}
Not only does the \mintinline{python}|BlockSeries| interface allow us to
implement the polynomial parametrization of the unitary transformation, but
also several other optimizations.
For example, we exploit Hermiticity when computing the Cauchy product of the
diagonal blocks of $\mathcal{U}$ and $\tilde{\mathcal{H}}$, because we only
compute half of the matrix products and then complex conjugate the result to
obtain the rest.
Similarly, we avoid computing $BA$ blocks of $\mathcal{U}$ and
$\tilde{\mathcal{H}}$ by providing a function to the \mintinline{python}|BlockSeries| that returns
the conjugate transpose of the respective $AB$ blocks.
As a result, whenever we query a $BA$ block, we first compute the $AB$ block,
store it, and then compute the $BA$ block directly.
This procedure brings an additional advantage: because the terms that compose
$AB$ blocks contain matrix products that first multiply small matrices and then
the large ones, it saves computational time and memory.
For example, the term $V_{n -i}^{AB} H_0^{BB}
W_i^{BB}$ in $Y_n$ is systematically computed as $(V_{n -i}^{AB}
H_0^{BB}) W_i^{BB}$ instead of $V_{n -i}^{AB}
(H_0^{BB} W_i^{BB})$.
This is only one example of how the \mintinline{python}|BlockSeries| interface allows us to
implement a symmetrized algorithm, and we leave other symmetries for future
work.
Such an extension would be useful for systems where $\mathcal{U}$ or
$\tilde{\mathcal{H}}$ vanish due to symmetries, so that the zero blocks can be
skipped beforehand.
\todo{Remove this paragraph?}

\co{To deal with an implicit B subspace, we use MUMPS and LinearOperators.}
Because \mintinline{python}|BlockSeries| can represent any input type, we use
it to directly implement the implicit algorithm for large sparse Hamiltonians.
To do this, we use the matrix $\Psi_A$ of the eigenvectors of the $A$ subspace
to rewrite the Hamiltonian as
%
\begin{align}
\mathcal{H} \to \begin{pmatrix}
\Psi_A^\dagger \mathcal{H} \Psi_A & \Psi_A^\dagger \mathcal{H} P_B \\
P_B \mathcal{H} \Psi_A & P_B \mathcal{H} P_B
\end{pmatrix},
\end{align}
%
where $P_B = 1 - \Psi_A \Psi_A^\dagger$ is the projector onto the $B$ subspace.
This Hamiltonian is larger in size than the original one because the $B$ block
has additional null vectors corresponding to the $A$ subspace.
This, however, allows to preserve the sparsity structure of the Hamiltonian by
applying $P_B$ and $\mathcal{H}$ separately.
Additionally, applying $P_B$ is efficient because $\Psi_A$ is a low rank matrix.
We then perform perturbation theory of the rewritten $\mathcal{H}$.
To solve the Sylvester's equation for the modified Hamiltonian, we write it for
every row of $V_n^{AB}$ separately:
%
\begin{align}
V_{n, ij}^{AB} (E_i - H_0) = Y_{n, j}
\end{align}
%
This equation is well-defined despite $E_i - H_0$ is not invertible because
$Y_{n}$ has no components in the $A$ subspace.
Here we also wrap the projector $P_B$ by a \mintinline{python}{LinearOperator}
object from \mintinline{python}{Scipy}.
This allows us to compute matrix-vector products between $P_B$ and a vector,
without explicitly constructing $P_B$ or any other product between elements of
the $B$ subspace, keeping the memory usage low.
For the same purpose, we use the MUMPS sparse solver
\cite{Amestoy_2001},
\cite{Amestoy_2006}, or the KPM method
\cite{Wei_e_2006}, to compute the Green's function of the
$B$ subspace.
As a consequence, the implicit algorithm can be used on matrices with millions
of degrees of freedom as long as they are sparse.

\co{Finally, we implement an overall function that interprets the user inputs and
returns a BlockSeries for the transformed Hamiltonian.}
On the other hand, the standard algorithm explicitly manipulates both subspaces,
but can work with dense matrices too.
We use the eigenvectors of the $A$ and $B$ subspaces to project the input
Hamiltonian and represent it with a \mintinline{python}|BlockSeries|, a
procedure that works for numerical and symbolic matrices.
Because we aim for an easy-to-use interface, we implement a function that
interprets the user inputs and decides which algorithm to use: the implicit
method if only the $A$ subspace is provided, and the standard algorithm
otherwise.
This is \mintinline{python}|block_diagonalize|, the only function that the user
needs to call.
If $H_0$ is diagonal and a custom function to solve Sylvester's equation is not
provided to \mintinline{python}|block_diagonalize|, Pymablock uses a default
function to compute the energy denominators.
