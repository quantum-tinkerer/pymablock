\section{Implementation}

\co{To implement the algorithms, we need a data structure that represents a
multidimensional series of block matrices.}
To implement the algorithm, we need a data structure that represents a
multidimensional series of operators, where dimensions label independent
perturbations.
Additionally, the data structure needs to label blocks, so that the algorithm
supports several forms of input, e.g. dense arrays, sparse matrices, symbolic
expressions, an implicit subspace, or a custom Python object that supports
products and sums.
Manipulating blocks also allows to compute the effective Hamiltonian without
explicitly constructing the full Hamiltonian, which is useful for Hamiltonians
with a large $BB$ subspace that is costly to store and compute.
To run the recursion, the series needs to be queryable by order and block.
This is also useful in cases where the user may want terms that combine
different perturbations, or when the user wants to compute more terms than
originally requested.
Lastly, the data structure needs to support a block-wise multivariate Cauchy
product, which is the main operation in the recursion and is used to compute
the transformed Hamiltonian.

\co{We address this by defining a BlockSeries class.}
To address these requirements, we define a \mintinline{python}|BlockSeries|
Python class and use it to represent the series of $\mathcal{U}$,
$\mathcal{H}$, and $\tilde{\mathcal{H}}$.
The objects of this class are equipped with a function to compute their elements
and a dictionary to cache the results.
For example, the \mintinline{python}|BlockSeries| for $\tilde{\mathcal{H}}$ has
a function that computes the block-wise multivariate Cauchy product in
Eq.~\eqref{eq:H_tilde_optimized}, \mintinline{python}|compute_H_tilde|.
%
\begin{minted}{python}
    H_tilde = BlockSeries(
        shape=(2, 2), # 2 blocks
        n_infinite=1, # number of perturbative parameters
        eval=compute_H_tilde,
        name="H_tilde",
        dimension_names=("lambda",),
    )
\end{minted}
%
To get the elements of the series, we implement Numpy array indexing,
which allows us to request several elements at once by using tuples and slices.
\mintinline{python}|H_tilde[0, 0, 2]| returns the $AA$ block of
$\tilde{\mathcal{H}}$ proportional to $\lambda^2$, and
\mintinline{python}|H_tilde[0, 0, :3]| returns the $AA$ block of orders
$\lambda^0$ , $\lambda^1$, and $\lambda^2$ in a Numpy masked array.
In the latter, the masked array only contains non-zero elements, a feature that
we use each time we compute the Cauchy products between series, avoiding
unnecessary matrix products.

\co{Using the BlockSeries interface allows us to implement a range of
optimizations that go beyond directly implementing the polynomial
parametrization}
Not only does the \mintinline{python}|BlockSeries| interface allow us to
implement the polynomial parametrization of the unitary transformation, but
also several other optimizations.
For example, we exploit Hermiticity when computing the Cauchy product of
$U'^{\dagger}U'$ in Eq.~\eqref{eq:W}, for which we only need to compute half of
the matrix products that form its diagonal blocks, and then complex conjugate
the result to obtain the rest.
Similarly, for Hermitian and anti-Hermitian series, like the off-diagonal
blocks of $\mathcal{X} - \mathcal{H}'_\textrm{offdiag}$ and off-diagonal blocks
of $\mathcal{U}'$, we only compute the $AB$ blocks, and use the conjugate
transpose to obtain the $BA$ blocks.
This is only one example of how the \mintinline{python}|BlockSeries| interface
allows us to implement a symmetrized algorithm, and we leave other symmetries
for future work.
Such an extension would be useful for systems where $\mathcal{U}$ or
$\tilde{\mathcal{H}}$ vanish due to symmetries, so that the zero blocks can be
skipped beforehand.

\co{A key optimization that BlockSeries enable is the implicit treatment
of the BB subspace.}
Solving Sylvester's equation and computing the matrix products are the most
expensive steps of the algorithms for large Hamiltonians.
Pymablock can efficiently construct an effective Hamiltonian of a small
subspace even when the full Hamiltonian is a sparse matrix that is too costly to
diagonalize.
It does so by avoiding explicit computation of operators in $B$ subspace, and
by utilizing the sparsity of the Hamiltonian to compute the Green's function,
which is the solution of Sylvester's equation.
To do so, Pymablock uses either the MUMPS sparse solver via the python-mumps
wrapper or the KPM method, an approach originally introduced in
references~\cite{Weisse_2006, Irfan_2019}.


\co{To deal with an implicit B subspace, we use MUMPS and LinearOperators.}
Because \mintinline{python}|BlockSeries| can represent any input type, we use
it to directly implement the implicit algorithm for large sparse Hamiltonians.
To do this, we use the matrix $\Psi_A$ of the eigenvectors of the $A$ subspace
to rewrite the Hamiltonian as
%
\begin{align}
\mathcal{H} \to \begin{pmatrix}
\Psi_A^\dagger \mathcal{H} \Psi_A & \Psi_A^\dagger \mathcal{H} P_B \\
P_B \mathcal{H} \Psi_A & P_B \mathcal{H} P_B
\end{pmatrix},
\end{align}
%
where $P_B = 1 - \Psi_A \Psi_A^\dagger$ is the projector onto the $B$ subspace.
This Hamiltonian is larger in size than the original one because the $B$ block
has additional null vectors corresponding to the $A$ subspace.
This, however, allows to preserve the sparsity structure of the Hamiltonian by
applying $P_B$ and $\mathcal{H}$ separately.
Additionally, applying $P_B$ is efficient because $\Psi_A$ is a low rank matrix.
We then perform perturbation theory of the rewritten $\mathcal{H}$.
To solve the Sylvester's equation for the modified Hamiltonian, we write it for
every row of $V_n^{AB}$ separately:
%
\begin{align}
V_{n, ij}^{AB} (E_i - H_0) = Y_{n, j}
\end{align}
%
This equation is well-defined despite $E_i - H_0$ is not invertible because
$Y_{n}$ has no components in the $A$ subspace.
Here we also wrap the projector $P_B$ by a \mintinline{python}{LinearOperator}
object from \mintinline{python}{Scipy}.
This allows us to compute matrix-vector products between $P_B$ and a vector,
without explicitly constructing $P_B$ or any other product between elements of
the $B$ subspace, keeping the memory usage low.
For the same purpose, we use the MUMPS sparse solver \cite{Amestoy_2001},
\cite{Amestoy_2006}, or the KPM method \cite{Weisse_2006}, to compute the
Green's function of the $B$ subspace.
As a consequence, the implicit algorithm can be used on matrices with millions
of degrees of freedom as long as they are sparse.

\co{Finally, we implement an overall function that interprets the user inputs and
returns a BlockSeries for the transformed Hamiltonian.}
On the other hand, the standard algorithm explicitly manipulates both subspaces,
but can work with dense matrices too.
We use the eigenvectors of the $A$ and $B$ subspaces to project the input
Hamiltonian and represent it with a \mintinline{python}|BlockSeries|, a
procedure that works for numerical and symbolic matrices.
Because we aim for an easy-to-use interface, we implement a function that
interprets the user inputs and decides which algorithm to use: the implicit
method if only the $A$ subspace is provided, and the standard algorithm
otherwise.
This is \mintinline{python}|block_diagonalize|, the only function that the user
needs to call.
If $H_0$ is diagonal and a custom function to solve Sylvester's equation is not
provided to \mintinline{python}|block_diagonalize|, Pymablock uses a default
function to compute the energy denominators.
