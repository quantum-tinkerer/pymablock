\section{Implementation}
\label{sec:implementation}

\subsection{The data structure for block operator series}
\label{sec:BlockSeries}

\co{To implement the algorithms, we need a data structure that represents a multidimensional series of block matrices.}
The optimized algorithm from the previous section requires constructing $14$ operator series, whose elements are computed using a collection of recurrence relations.
This warrants defining a specialized data structure suitable for this task that represents a multidimensional series of operators.
Because the recurrent relations are block-wise, the data structure needs to keep track of separate blocks.
In order to support varied use cases, the actual representation of the operators needs to be flexible: the block may be dense arrays, sparse matrices, symbolic expressions, or more generally any object that defines addition and multiplication.
Finally, the series needs to be queryable by order and block, so that it supports a block-wise multivariate Cauchy product---the main operation in the algorithm.

\co{A recursive implementation of the algorithm is better than an explicit loop over orders.}
The most straightforward way to implement a perturbation theory calculation is to write a function that has the desired order as an argument, computes the series up to that order, and returns the result.
This makes it hard to reuse already computed terms for a new computation, and becomes complicated to implement in the multidimensional case when different orders in different perturbations are needed.
We find that a recursive approach addresses these issues: within this paradigm, each series needs to define how its entries depend on lower-order terms.

\co{We address this by defining a BlockSeries class.}
To address these requirements, we define a \mintinline{python}|BlockSeries| Python class and use it to represent the series of $\mathcal{U}$, $\mathcal{H}$, and $\tilde{\mathcal{H}}$, as well as the intermediate series used to define the algorithm.
The objects of this class are equipped with a function to compute their elements and it stores the already computed results in a dictionary.
Storing the results for reuse is necessary to optimize the evaluation of higher order terms and it allows to request additional orders without restarting the computation.
For example, the definition of the \mintinline{python}|BlockSeries| for $\tilde{\mathcal{H}}$ has the following form:
%
\begin{minted}{python}
H_tilde = BlockSeries(
    shape=(2, 2),  # 2x2 block matrix
    n_infinite=n,  # number of perturbative parameters
    eval=compute_H_tilde,  # function to compute the elements
    name="H_tilde",
    dimension_names=("lambda", ...),  # parameter names
)
\end{minted}
%
Here \mintinline{python}|compute_H_tilde| is a function implementing Eq.~\eqref{eq:H_tilde_optimized} by querying other series objects.
Calling \mintinline{python}|H_tilde[0, 0, 2]|, the second order perturbation $\sim \lambda^2$ of the $AA$ block, then does the following:
\begin{enumerate}
    \item Evaluates \mintinline{python}|compute_H_tilde(0, 0, 2)| if it is not already computed.
    \item Stores the evaluation result in a dictionary.
    \item Returns the result.
\end{enumerate}
To conveniently access multiple orders at once, we implement NumPy array indexing so that \mintinline{python}|H_tilde[0, 0, :3]| returns a NumPy masked array array with the orders $\sim \lambda^0$ , $\sim \lambda^1$, and $\sim \lambda^2$ of the $AA$ block.
The masking allows to support a common use case where some orders of a series are zero, so that they are omitted from the computations.
We expect that the \mintinline{python}|BlockSeries| data structure is suitable to represent a broad class of perturbative calculations, and we plan to extend it to support more advanced features in the future.

\co{Using the BlockSeries interface allows us to implement a range of optimizations that go beyond directly implementing the polynomial parametrization}
We utilize \mintinline{python}|BlockSeries| to implement multiple other optimizations.
For example, we exploit Hermiticity when computing the Cauchy product of $U'^{\dagger}U'$ in Eq.~\eqref{eq:W}, by only evaluating half of the matrix products, and then complex conjugate the result to obtain the rest.
Similarly, for Hermitian and anti-Hermitian series, like the off-diagonal blocks of $\mathcal{U}'$, we only compute the $AB$ blocks, and use the conjugate transpose to obtain the $BA$ blocks.
This approach should also allow us to implement efficient handling of symmetry-constrained Hamiltonians, where some blocks either vanish or are equal to other blocks due to a symmetry.
Moreover, using \mintinline{python}|BlockSeries| with custom objects yields additional information about the algorithm and accommodates its further development.
Specifically, we have used a custom object with a counter to measure the algorithm complexity (see also Sec.~\ref{sec:benchmark}) and to determine which results are only used once so that they can be immediately discarded from storage.

\subsection{The implicit method for large sparse Hamiltonians}
\label{sec:implicit}

\co{Pymablock supports big sparse problems.}
A distinguishing feature of Pymablock is its ability to handle large sparse Hamiltonians, that are too costly to diagonalize, as illustrated in Sec.~\ref{sec:induced_gap}.
Specifically, we consider the situations when the size $N_E$ of the subspace of interest---explicit subspace---is small compared to the entire Hilbert space, so that obtaining the basis $\Psi_E$ of the explicit subspace is feasible using sparse diagonalization.
The projector on this subspace $P_E = \Psi_E^\dagger \Psi_E$ is then a low-rank matrix, a property that we exploit to avoid constructing the matrix representation of operators in the other, implicit, subspace.

\co{We use the extended sparsity-preserving Hilbert space and LinearOperator objects.}
The key tool to solve this problem is the projector approach introduced in Ref.~\cite{Irfan_2019}, which introduces an equivalent extended Hamiltonian using the projector $P_I = 1 - P_A$ onto the implicit subspace:
%
\begin{align}
\bar{\mathcal{H}} = \begin{pmatrix}
\Psi_E^\dagger \mathcal{H} \Psi_E & \Psi_E^\dagger \mathcal{H} P_I \\
P_I \mathcal{H} \Psi_E & P_I \mathcal{H} P_I
\end{pmatrix}.
\end{align}
%
In other words, the explicit subspace is written in the basis of $\Psi_E$, while the basis of the implicit subspace is the same as the original complete basis of $\mathcal{H}$ to preserve its sparsity.
The extended Hamiltonian projects out the $E$-degrees of freedom from the implicit subspace to avoid duplicate solutions in $\bar{\mathcal{H}}$, which introduces $N_E$ eigenvectors with zero eigenvalues.
Introducing $\bar{\mathcal{H}}$ allows to multiply by operators of a form $P_I H_\mathbf{n} P_I$ efficiently by using the low-rank structure of $P_E$.
In the code we represent the operators of the implicit subspace as \mintinline{python}|LinearOperator| objects from the SciPy package~\cite{Virtanen_2020}, enabled by the ability of the \mintinline{python}|BlockSeries| to store arbitrary objects.
Storing the remaining blocks of $\bar{\mathcal{H}}$ as dense matrices---efficient because these are small and dense---finishes the implementation of the Hamiltonian.

\co{We use sparse or KPM solvers to compute the Green's function of the $B$ subspace.}
To solve the Sylvester's equation we write it for every row of $V_{\mathbf{n}}^{EI}$ separately:
%
\begin{align}
V_{\mathbf{n}, ij}^{EI} (E_i - H_0) = Y^{EI}_{\mathbf{n}, j}
\end{align}
%
This equation has a solution despite $E_i - H_0$ not being invertible because $Y^{EI}_{\mathbf{n}} P_A = 0$.
We solve this equation using the MUMPS sparse solver~\cite{Amestoy_2001,Amestoy_2006}, which prepares an efficient sparse LU-decomposition of $E_i - H_0$, or the KPM approximation of the Green's function~\cite{Weisse_2006}.
Both methods work on sparse Hamiltonians with millions of degrees of freedom.

\subsection{Code generation}
\label{sec:codegen}

\co{To automate the implementation of the optimizations and allow further development of the algorithm, we design a code generation system.}
An efficient computation of a perturbative block-diagonalization requires a significant amount of repeated optimizations.
These include keeping track of the Hermiticity of involved series, applying the simpliciations due to block-diagonalization and the presence of only two blocks, or deletion of series terms that are only used once.
To separate the conceptual definition of the algorithm from these optimizations, we designed the code generation system that accepts a high-level description of the algorithm written in a domain-specific language and outputs the optimized Python code using the Python parser and the manipulation of the Python abstract syntax tree.
For example, the definition of the series $\mathcal{B}$ from Eqs.~(\ref{eq:B_diag},\ref{eq:B_offdiag}) is written as:
\begin{minted}{python}
with "B":
    start = 0
    if diagonal:
        ("U'† @ B" - "U'† @ B".adj + "H'_offdiag @ U'" + "H'_offdiag @ U'".adj) / -2
    if diagonal:
        zero if commuting_blocks[index[0]] else "V @ H'_diag" + "V @ H'_diag".adj
    if offdiagonal:
        -"U'† @ B"
\end{minted}
The corresponding compiled function for evaluating the terms of $\mathcal{B}$ begins with
\begin{minted}{python}
def series_eval(*index):
    which = linear_operator_series if use_linear_operator[index[:2]] else series
    result = zero
    if index[0] == index[1]:
        result = _zero_sum(
            result,
            diag(
                _safe_divide(
                    _zero_sum(
                        which["U'† @ B"][index], -Dagger(which["U'† @ B"][index]),
                        which["H'_offdiag @ U'"][index],
                        Dagger(which["H'_offdiag @ U'"][index]),
                    ), -2,
                ), index,
            ),
        )
    ...
\end{minted}
Here we only show the beginning of the generated function to illustrate the correspondence between the high-level description and the generated code.

\co{This enables further optimizations and extensions of the algorithm.}
The code generation system has accommodated multiple rewrites of the algorithm during the development.
We anticipate that it will enable treating different types of perturbative computations or other related algorithms, such as the derivative removal by adiabatic gate (DRAG) algorithm~\cite{Motzoi_2009,Theis_2018}.
Contrary to the perturbation theory setting, DRAG requires that the time-dependent Hamiltonian is block-diagonal in the rotating frame, and it achieves this goal by adding a series of corrections to the original Hamiltonian.
Its overall setting, however, is similar to time-dependent perturbation theory in that it amounts to solving a system of recurrent algebraic equations.
Our preliminary research already demonstrates that our code generation framework allows for a generalization of our work to the time-dependent perturbation theory, and we are confident that it applies to the DRAG algorithm as well.
